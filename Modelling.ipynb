{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:56:32.035858Z",
     "start_time": "2020-04-04T19:56:26.939707Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Flatten, Embedding, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydotplus as pyd\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:56:32.051789Z",
     "start_time": "2020-04-04T19:56:32.039073Z"
    }
   },
   "outputs": [],
   "source": [
    "def savetofile(obj,filename):\n",
    "    pickle.dump(obj,open(filename+\".p\",\"wb\"), protocol=4)\n",
    "\n",
    "def openfromfile(filename):\n",
    "    temp = pickle.load(open(filename+\".p\",\"rb\"))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:56:51.627386Z",
     "start_time": "2020-04-04T19:56:46.725708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>but they will have all those reviews</td>\n",
       "      <td>RoguishPoppet</td>\n",
       "      <td>ProductTesting</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 02:04:59</td>\n",
       "      <td>The dumb thing is, they are risking their sell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wow it is totally unreasonable to assume that ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 02:42:11</td>\n",
       "      <td>Clinton campaign accuses FBI of 'blatant doubl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ho ho ho but melania said that there is no way...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 16:20:53</td>\n",
       "      <td>Anyone else think that it was interesting the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i can not wait until potus starts a twitter wa...</td>\n",
       "      <td>kitduncan</td>\n",
       "      <td>politics</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 03:22:33</td>\n",
       "      <td>Here's what happens when Obama gives up his Tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>gotta love the teachers who give exams on the ...</td>\n",
       "      <td>DEP61</td>\n",
       "      <td>CFBOffTopic</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 03:30:11</td>\n",
       "      <td>Monday night Drinking thread Brought to You by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment         author  \\\n",
       "0      1               but they will have all those reviews  RoguishPoppet   \n",
       "1      1  wow it is totally unreasonable to assume that ...       pb2crazy   \n",
       "2      1  ho ho ho but melania said that there is no way...       pb2crazy   \n",
       "3      1  i can not wait until potus starts a twitter wa...      kitduncan   \n",
       "4      1  gotta love the teachers who give exams on the ...          DEP61   \n",
       "\n",
       "        subreddit  score  ups  downs     date          created_utc  \\\n",
       "0  ProductTesting      0   -1     -1  2016-11  2016-11-01 02:04:59   \n",
       "1        politics      2   -1     -1  2016-11  2016-11-01 02:42:11   \n",
       "2        politics      8   -1     -1  2016-10  2016-10-18 16:20:53   \n",
       "3        politics      3   -1     -1  2016-11  2016-11-01 03:22:33   \n",
       "4     CFBOffTopic      3   -1     -1  2016-11  2016-11-01 03:30:11   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  The dumb thing is, they are risking their sell...  \n",
       "1  Clinton campaign accuses FBI of 'blatant doubl...  \n",
       "2  Anyone else think that it was interesting the ...  \n",
       "3  Here's what happens when Obama gives up his Tw...  \n",
       "4  Monday night Drinking thread Brought to You by...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/clean/train.csv')\n",
    "cv = pd.read_csv('data/clean/cv.csv')\n",
    "test = pd.read_csv('data/clean/test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:56:51.647847Z",
     "start_time": "2020-04-04T19:56:51.629755Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    399994\n",
       "1    399978\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:56:51.659198Z",
     "start_time": "2020-04-04T19:56:51.652004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    49999\n",
       "1    49984\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:57:55.810196Z",
     "start_time": "2020-04-04T19:57:55.482683Z"
    }
   },
   "outputs": [],
   "source": [
    "train['comment'] = train['comment'].astype(str)\n",
    "cv['comment'] = cv['comment'].astype(str)\n",
    "test['comment'] = test['comment'].astype(str)\n",
    "\n",
    "train['author'] = train['author'].astype(str)\n",
    "cv['author'] = cv['author'].astype(str)\n",
    "test['author'] = test['author'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:58:18.376777Z",
     "start_time": "2020-04-04T19:57:58.442700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138775\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train['comment'].values)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T19:58:56.567436Z",
     "start_time": "2020-04-04T19:58:36.784040Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_comments_train = t.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = t.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = t.texts_to_sequences(test['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:06:06.686975Z",
     "start_time": "2020-04-04T20:06:06.517458Z"
    }
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for doc in encoded_comments_train:\n",
    "    lengths.append(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:06:07.381894Z",
     "start_time": "2020-04-04T20:06:07.301437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(lengths, 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:06:22.659084Z",
     "start_time": "2020-04-04T20:06:18.202789Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 70\n",
    "padded_comments_train = pad_sequences(encoded_comments_train, maxlen=max_length, padding='post')\n",
    "padded_comments_cv = pad_sequences(encoded_comments_cv, maxlen=max_length, padding='post')\n",
    "padded_comments_test = pad_sequences(encoded_comments_test, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:06:31.441382Z",
     "start_time": "2020-04-04T20:06:31.423905Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train['label'].values\n",
    "y_cv = cv['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_cv = to_categorical(y_cv, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:09:46.910493Z",
     "start_time": "2020-04-04T20:09:37.726935Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138775, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_w2v = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[word]\n",
    "    except:\n",
    "        embedding_vector = [0]*300\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:09:46.921897Z",
     "start_time": "2020-04-04T20:09:46.913407Z"
    }
   },
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:09:50.618205Z",
     "start_time": "2020-04-04T20:09:49.897768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " main_input (InputLayer)     [(None, 70)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 70, 300)           41632500  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 67, 50)            60050     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 33, 50)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 31, 100)           15100     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 15, 100)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1500)              0         \n",
      "                                                                 \n",
      " fully_connected (Dense)     (None, 100)               150100    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,857,952\n",
      "Trainable params: 225,452\n",
      "Non-trainable params: 41,632,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "out = Dense(2, activation='softmax')(dense)\n",
    "\n",
    "model_01 = Model(inputs=[input_data], outputs=[out])\n",
    "\n",
    "print(model_01.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(model_01, to_file='model_01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T20:10:06.151748Z",
     "start_time": "2020-04-04T20:10:06.146687Z"
    }
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='model_01')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', \n",
    "                              mode = 'max', \n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_01.h5\", \n",
    "                               monitor=\"val_f1_m\", \n",
    "                               mode=\"max\", \n",
    "                               save_best_only = True, \n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m', \n",
    "                            mode=\"max\", \n",
    "                            min_delta = 0, \n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-04T20:10:13.776Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.optimizers' has no attribute 'Adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1792/1499933922.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_01\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf1_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m h1 = model_01.fit(padded_comments_train, y_train, \n\u001b[0;32m      5\u001b[0m                \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.optimizers' has no attribute 'Adam'"
     ]
    }
   ],
   "source": [
    "c = optimizers.Adam(lr = 0.0001)\n",
    "model_01.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_m, 'acc'])\n",
    "\n",
    "h1 = model_01.fit(padded_comments_train, y_train, \n",
    "               batch_size=64, \n",
    "               epochs=50, \n",
    "               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n",
    "               validation_data=(padded_comments_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T17:53:44.523756Z",
     "start_time": "2020-03-10T17:53:32.285630Z"
    }
   },
   "outputs": [],
   "source": [
    "score_1 = model_01.evaluate(padded_comments_test, y_test)\n",
    "score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T17:53:58.137834Z",
     "start_time": "2020-03-10T17:53:47.493414Z"
    }
   },
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_01.predict(padded_comments_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T17:54:20.512945Z",
     "start_time": "2020-03-10T17:54:20.302758Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(h1.history['f1_m'][1:])\n",
    "plt.plot(h1.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h1.history['loss'][1:])\n",
    "plt.plot(h1.history['val_loss'][1:])\n",
    "plt.title('Model Los')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Baseline + Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = load_model('/kaggle/input/contextmodels/sentiment_model.h5', custom_objects={'f1_m': f1_m})\n",
    "sentiment_tokenizer = openfromfile('/kaggle/input/pickles/sentiment_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_comments_train_1 = sentiment_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv_1 = sentiment_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test_1 = sentiment_tokenizer.texts_to_sequences(test['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_comments_train_1 = pad_sequences(encoded_comments_train_1, maxlen=140, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv_1, maxlen=140, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test_1, maxlen=140, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_sentiment = 'fully_connected'\n",
    "intermediate_layer_model_1 = Model(inputs=sentiment_model.input, outputs=sentiment_model.get_layer(output_layer_sentiment).output)\n",
    "intermediate_output_train = intermediate_layer_model_1.predict(padded_comments_train_1)\n",
    "intermediate_output_cv = intermediate_layer_model_1.predict(padded_comments_cv_1)\n",
    "intermediate_output_test = intermediate_layer_model_1.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "\n",
    "input_sent = Input(shape=(100,), name='sentiment_input')\n",
    "concat = concatenate([dense, input_sent])\n",
    "\n",
    "out = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "model_02 = Model(inputs=[input_data, input_sent], outputs=[out])\n",
    "\n",
    "print(model_02.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(model_02, to_file='model_02.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='model_02')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', \n",
    "                              mode = 'max', \n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_02.h5\", \n",
    "                               monitor=\"val_f1_m\", \n",
    "                               mode=\"max\", \n",
    "                               save_best_only = True, \n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m', \n",
    "                            mode=\"max\", \n",
    "                            min_delta = 0, \n",
    "                            patience = 10,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = optimizers.Adam(lr = 0.0001)\n",
    "model_02.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_m, 'acc'])\n",
    "\n",
    "X_train = [padded_comments_train, np.array(intermediate_output_train)]\n",
    "X_cv = [padded_comments_cv, np.array(intermediate_output_cv)]\n",
    "X_test = [padded_comments_test, np.array(intermediate_output_test)]\n",
    "\n",
    "h2 = model_02.fit(X_train, y_train, \n",
    "               batch_size=64, \n",
    "               epochs=50, \n",
    "               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n",
    "               validation_data=(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2 = model_02.evaluate(X_test, y_test)\n",
    "score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_02.predict(X_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h2.history['f1_m'][1:])\n",
    "plt.plot(h2.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h2.history['loss'][1:])\n",
    "plt.plot(h2.history['val_loss'][1:])\n",
    "plt.title('Model Los')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Baseline + Emotion_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = load_model('/kaggle/input/contextmodels/emotion_model_1_1.h5', custom_objects={'f1_m': f1_m})\n",
    "emotion_tokenizer = openfromfile('/kaggle/input/pickles/emotion_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_comments_train = emotion_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = emotion_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = emotion_tokenizer.texts_to_sequences(test['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_comments_train_1 = pad_sequences(encoded_comments_train, maxlen=140, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv, maxlen=140, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test, maxlen=140, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_emotion = 'fully_connected'\n",
    "intermediate_layer_model = Model(inputs=emotion_model.input, outputs=emotion_model.get_layer(output_layer_emotion).output)\n",
    "intermediate_output_train = intermediate_layer_model.predict(padded_comments_train_1)\n",
    "intermediate_output_cv = intermediate_layer_model.predict(padded_comments_cv_1)\n",
    "intermediate_output_test = intermediate_layer_model.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "\n",
    "input_emo = Input(shape=(150,), name='emotion_input')\n",
    "concat = concatenate([dense, input_emo])\n",
    "\n",
    "out = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "model_03 = Model(inputs=[input_data, input_emo], outputs=[out])\n",
    "\n",
    "print(model_03.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(model_03, to_file='model_03.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='model_03')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', \n",
    "                              mode = 'max', \n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_03.h5\", \n",
    "                               monitor=\"val_f1_m\", \n",
    "                               mode=\"max\", \n",
    "                               save_best_only = True, \n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m', \n",
    "                            mode=\"max\", \n",
    "                            min_delta = 0, \n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = optimizers.Adam(lr = 0.0001)\n",
    "model_03.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_m, 'acc'])\n",
    "\n",
    "X_train = [padded_comments_train, np.array(intermediate_output_train)]\n",
    "X_cv = [padded_comments_cv, np.array(intermediate_output_cv)]\n",
    "X_test = [padded_comments_test, np.array(intermediate_output_test)]\n",
    "\n",
    "h3 = model_03.fit(X_train, y_train, \n",
    "               batch_size=64, \n",
    "               epochs=50, \n",
    "               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n",
    "               validation_data=(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_3 = model_03.evaluate(X_test, y_test)\n",
    "score_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_03.predict(X_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h3.history['f1_m'][1:])\n",
    "plt.plot(h3.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h3.history['loss'][1:])\n",
    "plt.plot(h3.history['val_loss'][1:])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Baseline + Emotion_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = load_model('/kaggle/input/contextmodels/emotion_model_2_1.h5', custom_objects={'f1_m': f1_m})\n",
    "emotion_tokenizer = openfromfile('/kaggle/input/pickles1/emotion_tokenizer_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_comments_train = emotion_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = emotion_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = emotion_tokenizer.texts_to_sequences(test['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_comments_train_1 = pad_sequences(encoded_comments_train, maxlen=300, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv, maxlen=300, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_emotion = 'fully_connected'\n",
    "intermediate_layer_model = Model(inputs=emotion_model.input, outputs=emotion_model.get_layer(output_layer_emotion).output)\n",
    "intermediate_output_train = intermediate_layer_model.predict(padded_comments_train_1)\n",
    "intermediate_output_cv = intermediate_layer_model.predict(padded_comments_cv_1)\n",
    "intermediate_output_test = intermediate_layer_model.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "\n",
    "input_emo = Input(shape=(150,), name='emotion_input')\n",
    "concat = concatenate([dense, input_emo])\n",
    "\n",
    "out = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "model_04 = Model(inputs=[input_data, input_emo], outputs=[out])\n",
    "\n",
    "print(model_04.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(model_04, to_file='model_04.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='model_04')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', \n",
    "                              mode = 'max', \n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_04.h5\", \n",
    "                               monitor=\"val_f1_m\", \n",
    "                               mode=\"max\", \n",
    "                               save_best_only = True, \n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m', \n",
    "                            mode=\"max\", \n",
    "                            min_delta = 0, \n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = optimizers.Adam(lr = 0.0001)\n",
    "model_04.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_m, 'acc'])\n",
    "\n",
    "X_train = [padded_comments_train, np.array(intermediate_output_train)]\n",
    "X_cv = [padded_comments_cv, np.array(intermediate_output_cv)]\n",
    "X_test = [padded_comments_test, np.array(intermediate_output_test)]\n",
    "\n",
    "h4 = model_04.fit(X_train, y_train, \n",
    "               batch_size=64, \n",
    "               epochs=50, \n",
    "               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n",
    "               validation_data=(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_4 = model_04.evaluate(X_test, y_test)\n",
    "score_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_04.predict(X_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h4.history['f1_m'][1:])\n",
    "plt.plot(h4.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h4.history['loss'][1:])\n",
    "plt.plot(h4.history['val_loss'][1:])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Baseline + Sentiment + Emotion_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = load_model('/kaggle/input/contextmodels/sentiment_model.h5', custom_objects={'f1_m': f1_m})\n",
    "sentiment_tokenizer = openfromfile('/kaggle/input/pickles/sentiment_tokenizer')\n",
    "\n",
    "encoded_comments_train = sentiment_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = sentiment_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = sentiment_tokenizer.texts_to_sequences(test['comment'])\n",
    "\n",
    "padded_comments_train_1 = pad_sequences(encoded_comments_train, maxlen=140, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv, maxlen=140, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test, maxlen=140, padding='post')\n",
    "\n",
    "output_layer_sentiment = 'fully_connected'\n",
    "intermediate_layer_model = Model(inputs=sentiment_model.input, outputs=sentiment_model.get_layer(output_layer_sentiment).output)\n",
    "intermediate_output_train_1 = intermediate_layer_model.predict(padded_comments_train_1)\n",
    "intermediate_output_cv_1 = intermediate_layer_model.predict(padded_comments_cv_1)\n",
    "intermediate_output_test_1 = intermediate_layer_model.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = load_model('/kaggle/input/contextmodels/emotion_model_1_1.h5', custom_objects={'f1_m': f1_m})\n",
    "emotion_tokenizer = openfromfile('/kaggle/input/pickles/emotion_tokenizer')\n",
    "\n",
    "encoded_comments_train = emotion_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = emotion_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = emotion_tokenizer.texts_to_sequences(test['comment'])\n",
    "\n",
    "padded_comments_train_1 = pad_sequences(encoded_comments_train, maxlen=140, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv, maxlen=140, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test, maxlen=140, padding='post')\n",
    "\n",
    "output_layer_emotion = 'fully_connected'\n",
    "intermediate_layer_model = Model(inputs=emotion_model.input, outputs=emotion_model.get_layer(output_layer_emotion).output)\n",
    "intermediate_output_train_2 = intermediate_layer_model.predict(padded_comments_train_1)\n",
    "intermediate_output_cv_2 = intermediate_layer_model.predict(padded_comments_cv_1)\n",
    "intermediate_output_test_2 = intermediate_layer_model.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "\n",
    "input_sent = Input(shape=(100,), name='sentimet_input')\n",
    "input_emo = Input(shape=(150,), name='emotion_input')\n",
    "concat = concatenate([dense, input_sent, input_emo])\n",
    "\n",
    "out = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "model_05 = Model(inputs=[input_data, input_sent, input_emo], outputs=[out])\n",
    "\n",
    "print(model_05.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(model_05, to_file='model_05.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='model_05')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', \n",
    "                              mode = 'max', \n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_05.h5\", \n",
    "                               monitor=\"val_f1_m\", \n",
    "                               mode=\"max\", \n",
    "                               save_best_only = True, \n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m', \n",
    "                            mode=\"max\", \n",
    "                            min_delta = 0, \n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = optimizers.Adam(lr = 0.0001)\n",
    "model_05.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_m, 'acc'])\n",
    "\n",
    "X_train = [padded_comments_train, np.array(intermediate_output_train_1), np.array(intermediate_output_train_2)]\n",
    "X_cv = [padded_comments_cv, np.array(intermediate_output_cv_1), np.array(intermediate_output_cv_2)]\n",
    "X_test = [padded_comments_test, np.array(intermediate_output_test_1), np.array(intermediate_output_test_2)]\n",
    "\n",
    "h5 = model_05.fit(X_train, y_train, \n",
    "               batch_size=64, \n",
    "               epochs=50, \n",
    "               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n",
    "               validation_data=(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_5 = model_05.evaluate(X_test, y_test)\n",
    "score_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_05.predict(X_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h5.history['f1_m'][1:])\n",
    "plt.plot(h5.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h5.history['loss'][1:])\n",
    "plt.plot(h5.history['val_loss'][1:])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Baseline + Sentiment + Emotion_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = load_model('/kaggle/input/contextmodels/sentiment_model.h5', custom_objects={'f1_m': f1_m})\n",
    "sentiment_tokenizer = openfromfile('/kaggle/input/pickles/sentiment_tokenizer')\n",
    "\n",
    "encoded_comments_train = sentiment_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = sentiment_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = sentiment_tokenizer.texts_to_sequences(test['comment'])\n",
    "\n",
    "padded_comments_train_1 = pad_sequences(encoded_comments_train, maxlen=140, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv, maxlen=140, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test, maxlen=140, padding='post')\n",
    "\n",
    "output_layer_sentiment = 'fully_connected'\n",
    "intermediate_layer_model = Model(inputs=sentiment_model.input, outputs=sentiment_model.get_layer(output_layer_sentiment).output)\n",
    "intermediate_output_train_1 = intermediate_layer_model.predict(padded_comments_train_1)\n",
    "intermediate_output_cv_1 = intermediate_layer_model.predict(padded_comments_cv_1)\n",
    "intermediate_output_test_1 = intermediate_layer_model.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = load_model('/kaggle/input/contextmodels/emotion_model_2_1.h5', custom_objects={'f1_m': f1_m})\n",
    "emotion_tokenizer = openfromfile('/kaggle/input/pickles1/emotion_tokenizer_1')\n",
    "\n",
    "encoded_comments_train = emotion_tokenizer.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = emotion_tokenizer.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = emotion_tokenizer.texts_to_sequences(test['comment'])\n",
    "\n",
    "padded_comments_train_1 = pad_sequences(encoded_comments_train, maxlen=300, padding='post')\n",
    "padded_comments_cv_1 = pad_sequences(encoded_comments_cv, maxlen=300, padding='post')\n",
    "padded_comments_test_1 = pad_sequences(encoded_comments_test, maxlen=300, padding='post')\n",
    "\n",
    "output_layer_emotion = 'fully_connected'\n",
    "intermediate_layer_model = Model(inputs=emotion_model.input, outputs=emotion_model.get_layer(output_layer_emotion).output)\n",
    "intermediate_output_train_2 = intermediate_layer_model.predict(padded_comments_train_1)\n",
    "intermediate_output_cv_2 = intermediate_layer_model.predict(padded_comments_cv_1)\n",
    "intermediate_output_test_2 = intermediate_layer_model.predict(padded_comments_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "\n",
    "input_sent = Input(shape=(100,), name='sentimet_input')\n",
    "input_emo = Input(shape=(150,), name='emotion_input')\n",
    "concat = concatenate([dense, input_sent, input_emo])\n",
    "\n",
    "out = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "model_06 = Model(inputs=[input_data, input_sent, input_emo], outputs=[out])\n",
    "\n",
    "print(model_06.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(model_06, to_file='model_06.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='model_06')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', \n",
    "                              mode = 'max', \n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_06.h5\", \n",
    "                               monitor=\"val_f1_m\", \n",
    "                               mode=\"max\", \n",
    "                               save_best_only = True, \n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m', \n",
    "                            mode=\"max\", \n",
    "                            min_delta = 0, \n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = optimizers.Adam(lr = 0.0001)\n",
    "model_06.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_m, 'acc'])\n",
    "\n",
    "X_train = [padded_comments_train, np.array(intermediate_output_train_1), np.array(intermediate_output_train_2)]\n",
    "X_cv = [padded_comments_cv, np.array(intermediate_output_cv_1), np.array(intermediate_output_cv_2)]\n",
    "X_test = [padded_comments_test, np.array(intermediate_output_test_1), np.array(intermediate_output_test_2)]\n",
    "\n",
    "h6 = model_06.fit(X_train, y_train, \n",
    "               batch_size=64, \n",
    "               epochs=50, \n",
    "               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n",
    "               validation_data=(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_6 = model_06.evaluate(X_test, y_test)\n",
    "score_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_06.predict(X_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h6.history['f1_m'][1:])\n",
    "plt.plot(h6.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h6.history['loss'][1:])\n",
    "plt.plot(h6.history['val_loss'][1:])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T08:35:40.344547Z",
     "start_time": "2020-04-06T08:35:40.339571Z"
    }
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = ['Model', 'F1-Score', '% of sarcastic comments correcly classified']\n",
    "x.add_row(['1', '0.7234', '73.58'])\n",
    "x.add_row(['2', '0.7179', '71.78'])\n",
    "x.add_row(['3', '0.7242', '71.75'])\n",
    "x.add_row(['4', '0.7235', '72.07'])\n",
    "x.add_row(['5', '0.7222', '70.71'])\n",
    "x.add_row(['6', '0.7215', '72.10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T08:35:40.466784Z",
     "start_time": "2020-04-06T08:35:40.463174Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
